{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "loaded-sympathy",
   "metadata": {},
   "source": [
    "# 6. 작사가 인공지능 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-forge",
   "metadata": {},
   "source": [
    "목차\n",
    "시퀀스? 스퀀스!\n",
    "I 다음 am을 쓰면 반 이상은 맞더라\n",
    "실습\n",
    "1) 데이터 다듬기\n",
    "2) 인공지능 학습시키기\n",
    "3) 잘 만들어졌는지 평가하기\n",
    "프로젝트 : 멋진 작사가 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "korean-buyer",
   "metadata": {},
   "source": [
    "# 6-2. 시퀀스? 스퀀스!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "least-shell",
   "metadata": {},
   "source": [
    "## 시퀀스\n",
    "- 시퀀스는 데이터에 순서(번호)를 붙여 나열한 것\n",
    "  \n",
    "- 특징:\n",
    "    - 데이터를 순서대로 하나씩 나열하여 나타낸 구조다\n",
    "    - 특정 위치(~번째) 데이터를 가리킬 수 있다.\n",
    "    \n",
    "시퀀스가 담은 데이터 순서는 정렬이 아닌 나열이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decimal-myrtle",
   "metadata": {},
   "source": [
    "# 6-3. I 다음 am을 쓰면 반 이상은 맞더라"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outstanding-drilling",
   "metadata": {},
   "source": [
    "나는 밥을 [ ] = 먹는다 ; 밥은 통계적으로 먹힘\n",
    "알바생이 커피를 [ ] = 만든다 ; 알바생은 통계적으로 커피를 만듦\n",
    "\n",
    "인공지능이 글을 읽는 방식도 같다. 문법적인 원리를 통해서가 아닌 수많은 글을 읽게 함으로써 나는, 밥을, 뒤에는 먹는다가, 알바생이, 커피를, 뒤에는 만든다라느 사실을 알게 하는 것.\n",
    "  \n",
    "그런 이유에서 많은 데이터가 곧 좋은 결과를 만들어냄\n",
    "  \n",
    "이 방식을 제일 잘 처리하는 인공지능 중 하나가 순환신경망(RNN)임\n",
    "![img](https://aiffelstaticprd.blob.core.windows.net/media/images/E-12-RNN2.max-800x600.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "running-warren",
   "metadata": {},
   "source": [
    "나는 밥을 [ ] = 먹는다가 들어가야 하는 것을 배웠지만  \n",
    "첫 시작인 '나는'은 어떻게 만드나?\n",
    "  \n",
    "\\<strat> 라는 특수한 토큰을 맨 앞에 추가해주므로써 해결 가능. \\<start>는 \"이제 문장을 생성해봐\"라고 사인을 주는 것  \n",
    "\\<strat>를 입력 받은 순환신경망(RNN)은 다음 단어로 '나는'을 생성하고, **생성한 단어를 다음 입력으로 사용**  \n",
    "이런 순환적인 특성을 살려 이름이 순환신경망(RNN)\n",
    "    \n",
    "그렇게 순차적으로 '밥을 먹었다'까지 생성하고 나면 인공지능은 \"다 만들었다\"는 신호로 \\<end> 토큰을 생성.  \n",
    "즉, 우리는 \\<strat>가 문장의 시작에 더해진 데이터(**문제지**)와 \\<end>가 문장의 끝에 더해진 출력(**답안지**)가 필요함  \n",
    "이는 문장 데이터만 있으면 만들어낼 수 있다는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ordinary-active",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source 문장 :  <start>나는 밥을 먹었다\n",
      "Target 문장 :  나는 밥을 먹었다<end>\n"
     ]
    }
   ],
   "source": [
    "sentence = \"나는 밥을 먹었다\"\n",
    "\n",
    "source_sentence = \"<start>\" + sentence\n",
    "target_sentence = sentence + '<end>'\n",
    "\n",
    "print(\"Source 문장 : \", source_sentence)\n",
    "print(\"Target 문장 : \", target_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulated-philadelphia",
   "metadata": {},
   "source": [
    "# 언어 모델 (Language Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-example",
   "metadata": {},
   "source": [
    "나는, 밥을, 먹었다를 순차적으로 생성할 때 '밥을' 다음이 '먹었다'인 것은 쉽게 알 수 있다.  \n",
    "하지만 '나는' 다음이 '밥을'인 것은 조금 억지스러움  \n",
    "실제 동작 방식에서도 '나는' 다음이 '밥을'이 나오는 것은 순전히 운\n",
    "\n",
    "## 사건 B가 일어나는 경우에 사건 A가 일어날 확률은 P(A \\vert B)P(A∣B)로 표기\n",
    "  \n",
    "확률적 표현:  \n",
    "'나는 밥을' 다음에 '먹었다'가 나올 확률을 p(먹었다|나는 밥을)  \n",
    "그렇다면 이 확률은 '나는' 뒤에 '밥이'가 나올 확률인 p('밥이'|'나는')보다는 높게 나올 것  \n",
    "아마 p(먹었다|나는, 밥을, 맛있게)의 확률값은 더 높을 것  \n",
    "어떤 문구 뒤에 다음 단어가 나올 확률이 높다는 것은 다음 단어가 나오는 것이 보다 자연스럽다는 뜻  \n",
    "나는 뒤에 밥을이 나오는 것이 자연스럽지 않다는 것이 아니라 나는 뒤에는 올 수 있는 자연스러운 단어의 경우의 수가 너무 많다보니 불확실성이 높을 뿐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-automation",
   "metadata": {},
   "source": [
    "n-1개의 단어 시퀀스 w_1,...,w_n-1가 주어졌을 때 n번째 단어 w_n으로 무엇이 올지를 예측하는 확률 모델을 언어모델(Language Model)이라고 부름  \n",
    "P(w_n|w_1,...,w_n-1;θ) : w_n-1 다음에 w_n이 나올 확률  \n",
    "n-1번재까지의 단어 시퀀스가 x_train이 되고 n번째 단어가 y_train\n",
    "w_1, ... , w_n-1가 주어졌을 때 n번재 단어 w_n으로 무엇이 올지 예측하는 구조를 가지고 있음을 알아챌 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tight-picnic",
   "metadata": {},
   "source": [
    "# 6-4. 실습 (1) 데이터 다듬기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "formal-sense",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First Citizen:', 'Before we proceed any further, hear me speak.', '', 'All:', 'Speak, speak.', '', 'First Citizen:', 'You are all resolved rather to die than to famish?', '']\n"
     ]
    }
   ],
   "source": [
    "import re       # 정규표현식을 위한 Regax 지원 모듈(문장 데이터 정돈에 필요)\n",
    "import numpy as np       # 변환된 문장 데이터(행렬)을 편하게 처리하기 위해\n",
    "import tensorflow as tf   # 텐서플로우\n",
    "import os\n",
    "\n",
    "# 파일 읽기모드로 열기\n",
    "file_path = os.getenv(\"HOME\") + '/aiffel/lyricist/data/shakespeare.txt'\n",
    "with open(file_path, 'r') as f:\n",
    "    raw_corpus = f.read().splitlines() # 텍스트를 라인 단위로 끊어서 list 형태로 불러옴\n",
    "    \n",
    "print(raw_corpus[:9]) # 앞에서부터 10라인만 화면에 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-communist",
   "metadata": {},
   "source": [
    "### 우린 문장(대사)만을 원하므로 화자 이름이나 공백뿐인 정보는 필요가 없음\n",
    "#### 1차 필터링\n",
    "![img](https://aiffelstaticprd.blob.core.windows.net/media/images/E-12-3.data.max-800x600.png)\n",
    "  \n",
    "### 원치 않는 문장  \n",
    "#### 화자가 표기된 문장(0, 3, 6), 공백인(2, 5, 9)  \n",
    "##### 화자가 표기된 문장은 문장의 끝이 ':'로 끝남. 일반적으로 대사가 ':'로 끝날일이 없으니 ':'를 제외시켜도 괜찮을듯\n",
    "##### 공백인 문장은 길이를 검사하여 길이가 0이라면 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "brazilian-continuity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before we proceed any further, hear me speak.\n",
      "Speak, speak.\n",
      "You are all resolved rather to die than to famish?\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue # 길이가 0인 문장은 건너뛰고\n",
    "    if sentence[-1] == \":\": continue # 문장의 끝이 :이면 건너뛰고\n",
    "    \n",
    "    if idx >9: break # 일단 문장 10개에서 끝내\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "productive-turkish",
   "metadata": {},
   "source": [
    "### 텍스트 생성 모델에도 단어사전을 만들게 됨\n",
    "### 그렇다면 문장을 일정한 기준으로 쪼개야함\n",
    "### 일정한 기준으로 쪼개는 과정을 토큰화(Tokenize) 라고 함\n",
    "  \n",
    "### 문제케이스  \n",
    "1. Hi, my name is John. ('hi', 'my', ... 'john.')으로 분리됨 - 문장부호  \n",
    "2. First, epn the first chapter. (First와 first 다른 단어로 인식) - 대소문자  \n",
    "3. He is a ten-year-old boy. (ten-year-old를 한 단어로 인식) - 특수무자  \n",
    "  \n",
    "'1.'을 막기 위해 문장 부호 양쪽에 공백을 추가  \n",
    "'2.'를 막기 위해 모든 문자들을 소문자로 변환  \n",
    "'3.'을 막기 위해 특수문자들은 모두 제거\n",
    "\n",
    "## 이런 전처리 정규표현식(Regex)을 이용한 필터링이 유용하게 사용됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "addressed-psychiatry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()       # 소문자로 바꾸고 양쪽 공백을 삭제\n",
    "  \n",
    "    # 아래 3단계를 거쳐 sentence는 스페이스 1개를 delimeter(구분문자)로 하는 소문자 단어 시퀀스로 바뀝니다.\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)        # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)                  # 공백 패턴을 만나면 스페이스 1개로 치환\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)  # a-zA-Z?.!,¿ 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    sentence = '<start> ' + sentence + ' <end>'      # 이전 스텝에서 본 것처럼 문장 앞뒤로 <start>와 <end>를 단어처럼 붙여 줍니다\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))   # 이 문장이 어떻게 필터링되는지 확인해 보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confident-static",
   "metadata": {},
   "source": [
    "# 데이터 전처리 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documented-management",
   "metadata": {},
   "source": [
    "## 우리가 구축해야할 데이터 셋은 어떤 모양일까?\n",
    "자연어 처리 분야에서  \n",
    "소스문장(Source sentence) : 모델의 입력이 되는 문장 : x_train  \n",
    "타겟문장(Target sentence) : 정답 역할을 하게 될 모델의 출력 문장 : y_train    \n",
    "    \n",
    "위에서 만든 정제 함수를 통해 만든 데이터셋 토큰화를 진행한 후  \n",
    "끝 단어 '<end>'를 없애면 소스문장  \n",
    "첫 단어 '<start>'를 없애면 타겟문장이 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "liberal-smith",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> before we proceed any further , hear me speak . <end>',\n",
       " '<start> speak , speak . <end>',\n",
       " '<start> you are all resolved rather to die than to famish ? <end>',\n",
       " '<start> resolved . resolved . <end>',\n",
       " '<start> first , you know caius marcius is chief enemy to the people . <end>',\n",
       " '<start> we know t , we know t . <end>',\n",
       " '<start> let us kill him , and we ll have corn at our own price . <end>',\n",
       " '<start> is t a verdict ? <end>',\n",
       " '<start> no more talking on t let it be done away , away ! <end>',\n",
       " '<start> one word , good citizens . <end>']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정제함수 활용하여 정제 데이터 구축\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:            # row_corpus를 Sentece에 반복해서 넣어라\n",
    "    if len(sentence) == 0: continue    # sentence길이가 0이면 건너뛰고\n",
    "    if sentence[-1] == \":\": continue   # sentece 끝이 : 이면 건너뛰고\n",
    "        \n",
    "    corpus.append(preprocess_sentence(sentence))   # courpus에 추가해라 만든 정제함수(preprocess_sentece)에 sentence를 넣어서\n",
    "    \n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "large-poetry",
   "metadata": {},
   "source": [
    "### 텍서플로우는 자연어 처리를 위한 여러가지 모듈을 제공\n",
    "#### tf.keras.preprocessing.text.Tokenizer 패키지는 \n",
    "#### 1. 정제된 데이터를 토큰화하고\n",
    "#### 2. 단어사전(vocabulary, dictionary라고 부름)을 만들어주며\n",
    "#### 3. 데이터를 숫자로 변환시켜줌\n",
    "\n",
    "#### 이 과정을 벡터화(vetorize)라 하며, 숫자로 변환된 데이터를 텐서(tensor)라고 함\n",
    "#### 우리가 사용하는 텐서플로우로 만든 모델의 입출력 데이터는 실제로 모두 이런 텐서 변환되어 처리 되는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "formal-nowhere",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Type</th>\n",
       "      <th>Example</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>scalar</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>vector</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>matrix</td>\n",
       "      <td>[[1, 1], [1, 1]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3-tensor</td>\n",
       "      <td>[[[1, 1], [1, 1]], [[1, 1], [1, 1]], [[1, 2], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>n</td>\n",
       "      <td>n-tensor</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Rank      Type                                            Example\n",
       "0    0    scalar                                                [1]\n",
       "1    1    vector                                             [1, 1]\n",
       "2    2    matrix                                   [[1, 1], [1, 1]]\n",
       "3    3  3-tensor  [[[1, 1], [1, 1]], [[1, 1], [1, 1]], [[1, 2], ...\n",
       "4    n  n-tensor                                                 []"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "\n",
    "df1 = df(data={\"Rank\": [\"0\", \"1\", \"2\", \"3\", \"n\"], \"Type\": [\"scalar\", \"vector\", \"matrix\", \"3-tensor\", \"n-tensor\"], \n",
    "               \"Example\": [[1], [1, 1], [[1, 1], [1,1]], [[[1,1],[1,1]],[[1,1],[1,1]],[[1,2],[2,1]]], []]},\n",
    "        columns = ['Rank', \"Type\", \"Example\"])\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romantic-fashion",
   "metadata": {},
   "source": [
    "스칼라는 일반적오르 존재하는 그냥 값 1개.  \n",
    "벡터는 스칼라가 여러개 모인 것이며 차원이 높아질 수록 아래 차원의 것을 모아 놓은 배열이라고 할 수 있음\n",
    "![img](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FqS1NF%2FbtqubStze09%2F5sbnC8DVd3aQsUULgjv6Kk%2Fimg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "provincial-biology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2  143   40 ...    0    0    0]\n",
      " [   2  110    4 ...    0    0    0]\n",
      " [   2   11   50 ...    0    0    0]\n",
      " ...\n",
      " [   2  149 4553 ...    0    0    0]\n",
      " [   2   34   71 ...    0    0    0]\n",
      " [   2  945   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7fae9199aad0>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):  \n",
    "    # 텐서플로우에서 제공하는 Tokenizer패키지 생성\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words = 7000, # 전체 단어 개수\n",
    "        filters=' ', # 별도로 전처리 로직을 추가할 수 있지만 이번에는 사용 안 함\n",
    "        oov_token=\"<unk>\" # out-of-vocabulary, 사전에 없었던 단어는 어떤 토큰으로 대체할지\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus) # 우리가 구축한 corpus로부터 Tokenize가 사전을 자동 구축\n",
    "    \n",
    "    # 이후 tokenizer를 활용하여 모델에 입력할 데이터셋 구축\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)  # tokenizer는 구축한 사전으로부터 corpus를 해석새 Tensor로 변환\n",
    "    \n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기 위한 padding 메소드 제공\n",
    "    # maxlen의 디폴트 값은 None. 이 경우 corpus의 가장 긴 문장을 기준으로 시퀀스 길이가 맞춰짐\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    \n",
    "    print(tensor, tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "convinced-script",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2  143   40  933  140  591    4  124   24  110]\n",
      " [   2  110    4  110    5    3    0    0    0    0]\n",
      " [   2   11   50   43 1201  316    9  201   74    9]]\n"
     ]
    }
   ],
   "source": [
    "# 생성된 텐서 데이터를 3번째 행, 10번째 열까지만 출력\n",
    "print(tensor[:3,:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "continuous-breed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : .\n",
      "6 : the\n",
      "7 : and\n",
      "8 : i\n",
      "9 : to\n",
      "10 : of\n"
     ]
    }
   ],
   "source": [
    "# 텐서데이터는 모두 정수로 이루어짐\n",
    "# 이 숫자는 tokenizer에 구축된 단어 사전의 인덱스\n",
    "# 단어 사전 어떻게 구축되었는지 확인\n",
    "\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "    \n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-looking",
   "metadata": {},
   "source": [
    "# 2번째 인덱스가 <start>여서 모든 행이 2로 시작하는 것을 알 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "union-juice",
   "metadata": {},
   "source": [
    "### 위에 텐서 출력부에서 행 뒤쪽에 0이 많이 나온 부분은 저해진 입력 시퀀스 길이보다 문장이 짧을경우 0으로 패딩(padding)을 채워 넣은 것\n",
    "### 사전에는 없지만 0은 바로 패딩 문자<pad>가 될 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "available-sandwich",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2 143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0\n",
      "   0   0]\n",
      "[143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0   0\n",
      "   0   0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1] # tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성. 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높음\n",
    "tgt_input = tensor[:, 1:] # tensor에서 <satrt>를 잘라내서 타겟문장 생성\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peaceful-wichita",
   "metadata": {},
   "source": [
    "### corpus내의 첫번째 문장에 대해 생성된 소스와 타겟 문장 확인\n",
    "### 예상대로 소스는 2\\<start>에서 시작해서 3\\<end>로 끝난 후 0\\<pad>로 채워져 있음\n",
    "### 하지만 타겟은 2로 시작하지않고 소스를 오니쪽으로 한칸 시프트한 형태를 보임(143 부터 시작)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-resistance",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 데이터셋 객체 생성\n",
    "### 그동안 model.fit(x_train, y_train, ...) 형태로 Numpy Array데이터셋을 생성하여 model에 제공하는 형태의 학습을 진행했음\n",
    "### 그러나 텐서플로우를 활용할 경우 텐서로 생성된 데이터를 이용해 tf.data.Dataset 객체를 많이 사용함\n",
    "### rf.data.Dataset객체는 텐서플로우에서 활용할 경우 데이터 입력 파이프라인을 통한 속도 개선 및 각종 편의기능을 제공하므로 사용법 알아둘 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dental-surge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 우리는 이미 데이터셋을 텐서 형테로 생성해 두었으므로 tf.data.Dataset.from_tensor_slices() 메소드를 이용해 tf.data.Dataset 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "peaceful-beginning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 20), (256, 20)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1 # tokenizer가 구축된 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>!\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-mozambique",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 과정 기억\n",
    "- 정규표현식을 이용한 corpus 생성\n",
    "- tf.kera.preprocessing.text.Tokenizer를 이용해 corpus를 텐서로 변환\n",
    "- tf.data.Dataset.from_tensor_slices()를 이용해 corpus텐서를 tf.data.Dataset객체로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "national-liabilities",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset을 얻음으로써 데이터 다듬기 과정 끝\n",
    "# tf.data.Dataset에서 제공하는 shuffle(), batch() 등 다양한 데이터셋 관련 기능을 손쉽게 이용 가능\n",
    "# 이 모든 과정을 텐서플로우의 데이터 전처리 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-parallel",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-factor",
   "metadata": {},
   "source": [
    "# 6-5. 실습 (2) 인공지능 학습시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stainless-camcorder",
   "metadata": {},
   "source": [
    "# 우리가 만들 모델 구조도\n",
    "![Img](https://aiffelstaticprd.blob.core.windows.net/media/images/E-12-4.max-800x600.png)\n",
    "#### 우리가 만들 모델은 tf.keras.Model을 Subclassing하는 방식으로 만들 거임\n",
    "#### 위 그림에서 처럼 우리가 만들 모델에는 1개의 Embedding레이어, 2개의 LSTM레이어, 1개의 DENSE레이어로 구성되어 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "floppy-collective",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256 #embedding_size는 워드 벡터의 차원수, 즉 단어가 추상적으로 표현되는 크기\n",
    "hidden_size = 1024 # hidden_size는 모델에 얼마나 많은 일꾼을 둘 것인가 정도로 이해하면 됨\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size, hidden_size)\n",
    "\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-australia",
   "metadata": {},
   "source": [
    "#### 입력 텐서에는 단어 사전의 인덱스가 들어있음\n",
    "#### Embedding 레이어는 이 인덱스 값을 해당 인덱스 번째의 워드 벡터로 바꿔줌\n",
    "#### 이 워드 벡터는 공간에서 단어의 추상적 표현으로 사용됨\n",
    "  \n",
    "#### 위 코드에서 embedding_size는 워드 벡터의 차원수, 즉 단어가 추상적으로 표현되는 크기\n",
    "#### 만약 그 크기가 2라면\n",
    "- 차갑다:[0.0, 1.0]\n",
    "- 뜨겁다:[1.0, 0.0]\n",
    "- 미지근하다:[0.5, 0.5]\n",
    "#### 정도로 구분이 가능함\n",
    "#### embedding_size 값이 커질수록 단어의 추상적인 특징들을 더 잡아낼 수 있지만, 그만큼 충분한 데이터가 주어지지 않으면 혼란을 야기\n",
    "#### 이번 실습에선 256이 적당해보임\n",
    "  \n",
    "#### LSTM레이어의 hidden state의 차원수인 hidden_size도 같은 맥락\n",
    "#### hidden_size는 모델에 얼마나 많은 일꾼을 둘 것인가 정도로 이해하면 됨\n",
    "#### 마찬가지로 충분한 데이터가 주어지면 올바른 경정을 내리지만 그렇지 않으면 혼란을 야기함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-mattress",
   "metadata": {},
   "source": [
    "#### 우리의 model은 아직 제대로 build되지 않음\n",
    "#### model.compile()을 호출한 적도 없고\n",
    "#### 입력텐서가 무엇인지 제대로 지정도 안해줌\n",
    "#### 그런경우 아래와 같이 model에 데이터를 아주 조금 태워보는 것도 방법\n",
    "#### model의 input shape까 결정되면서 model.build까 자동 호출 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "hispanic-interstate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 20, 7001), dtype=float32, numpy=\n",
       "array([[[ 3.02529457e-04,  5.12329570e-05, -7.60959665e-05, ...,\n",
       "          8.18049216e-07,  2.62600573e-04,  5.22834671e-05],\n",
       "        [ 4.41622105e-04, -1.85308512e-04, -1.35317663e-04, ...,\n",
       "         -5.73762263e-05, -9.03309774e-05,  2.18827263e-04],\n",
       "        [ 2.61754409e-04, -3.57642042e-04, -3.60564765e-04, ...,\n",
       "         -1.80342176e-04, -3.41744279e-04,  4.58278955e-04],\n",
       "        ...,\n",
       "        [-9.27285175e-04,  1.08358763e-04,  9.16174205e-04, ...,\n",
       "          3.14459298e-03, -1.67383137e-03, -1.72942516e-03],\n",
       "        [-8.86218855e-04, -6.15459567e-06,  1.01439608e-03, ...,\n",
       "          3.54310567e-03, -1.88168488e-03, -2.13172939e-03],\n",
       "        [-7.93857093e-04, -1.19667275e-04,  1.09925866e-03, ...,\n",
       "          3.89152835e-03, -2.03005224e-03, -2.47139996e-03]],\n",
       "\n",
       "       [[ 3.02529457e-04,  5.12329570e-05, -7.60959665e-05, ...,\n",
       "          8.18049216e-07,  2.62600573e-04,  5.22834671e-05],\n",
       "        [ 5.99050312e-04,  2.74614518e-04,  3.39466387e-05, ...,\n",
       "         -1.80504547e-04,  3.75160424e-04, -1.69412760e-05],\n",
       "        [ 1.03549729e-03,  4.00001969e-04,  9.52051050e-05, ...,\n",
       "         -4.03797225e-04,  4.33705252e-04, -1.06408166e-04],\n",
       "        ...,\n",
       "        [-5.77139144e-04,  4.69230319e-04,  6.36459095e-04, ...,\n",
       "          3.47695989e-03, -1.41033716e-03, -1.55835354e-03],\n",
       "        [-5.56039857e-04,  3.54507822e-04,  6.89087552e-04, ...,\n",
       "          3.91578441e-03, -1.60284713e-03, -1.99273834e-03],\n",
       "        [-4.91838320e-04,  2.33629937e-04,  7.61214411e-04, ...,\n",
       "          4.27487493e-03, -1.75240182e-03, -2.35986779e-03]],\n",
       "\n",
       "       [[ 3.02529457e-04,  5.12329570e-05, -7.60959665e-05, ...,\n",
       "          8.18049216e-07,  2.62600573e-04,  5.22834671e-05],\n",
       "        [ 2.84523994e-04,  1.61829721e-05, -2.37155444e-04, ...,\n",
       "         -7.63051867e-05,  2.61343055e-04,  3.24692701e-05],\n",
       "        [-2.99043113e-05, -2.25132608e-04, -6.39961858e-04, ...,\n",
       "         -4.74980334e-04,  5.80893720e-05,  5.58856584e-04],\n",
       "        ...,\n",
       "        [-2.56839248e-05, -9.33580799e-04,  1.04876026e-03, ...,\n",
       "          4.93644597e-03, -2.11240537e-03, -3.34997498e-03],\n",
       "        [ 1.09401306e-04, -9.16544523e-04,  1.19465124e-03, ...,\n",
       "          5.08469902e-03, -2.14954326e-03, -3.45606310e-03],\n",
       "        [ 2.32258259e-04, -8.96343612e-04,  1.33162201e-03, ...,\n",
       "          5.20533975e-03, -2.17069080e-03, -3.53313610e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 3.02529457e-04,  5.12329570e-05, -7.60959665e-05, ...,\n",
       "          8.18049216e-07,  2.62600573e-04,  5.22834671e-05],\n",
       "        [ 5.99050312e-04,  2.74614518e-04,  3.39466387e-05, ...,\n",
       "         -1.80504547e-04,  3.75160424e-04, -1.69412760e-05],\n",
       "        [ 7.87750585e-04,  5.29518817e-04,  4.70894563e-04, ...,\n",
       "         -2.34540727e-04,  4.93159227e-04, -2.81374116e-04],\n",
       "        ...,\n",
       "        [-5.12570667e-04,  2.64395319e-04,  8.50737386e-04, ...,\n",
       "          2.78926594e-03, -1.21348468e-03, -2.40811170e-03],\n",
       "        [-4.70626517e-04,  1.51379863e-04,  9.92903137e-04, ...,\n",
       "          3.30659142e-03, -1.41124160e-03, -2.69861147e-03],\n",
       "        [-3.92947026e-04,  4.03756603e-05,  1.12290226e-03, ...,\n",
       "          3.73513531e-03, -1.56440423e-03, -2.94811395e-03]],\n",
       "\n",
       "       [[ 3.02529457e-04,  5.12329570e-05, -7.60959665e-05, ...,\n",
       "          8.18049216e-07,  2.62600573e-04,  5.22834671e-05],\n",
       "        [ 4.06688458e-04,  5.25213487e-04,  4.49393592e-05, ...,\n",
       "         -2.04682423e-04,  1.74914428e-04,  2.48530094e-04],\n",
       "        [ 3.90907080e-04,  8.08159471e-04, -4.91347600e-05, ...,\n",
       "         -1.49352360e-04,  4.69457620e-04,  6.29075745e-04],\n",
       "        ...,\n",
       "        [-3.89531197e-04, -2.31840371e-04,  4.15628660e-04, ...,\n",
       "          3.74578405e-03, -1.70866610e-03, -2.18651164e-03],\n",
       "        [-3.17094265e-04, -3.17813916e-04,  5.66045113e-04, ...,\n",
       "          4.11769701e-03, -1.82928063e-03, -2.55111302e-03],\n",
       "        [-2.18536079e-04, -4.02372389e-04,  7.06728897e-04, ...,\n",
       "          4.42371378e-03, -1.91953126e-03, -2.84243654e-03]],\n",
       "\n",
       "       [[ 3.02529457e-04,  5.12329570e-05, -7.60959665e-05, ...,\n",
       "          8.18049216e-07,  2.62600573e-04,  5.22834671e-05],\n",
       "        [-4.99868074e-05,  2.83083500e-04, -9.87065141e-05, ...,\n",
       "          1.91463565e-04,  3.04544024e-04, -3.16057012e-05],\n",
       "        [-4.71204228e-04,  3.18433711e-04, -1.06258367e-05, ...,\n",
       "          1.85916462e-04,  3.34976125e-04, -2.91867182e-04],\n",
       "        ...,\n",
       "        [-2.49265169e-04, -7.62807147e-04,  1.17344898e-03, ...,\n",
       "          4.54361038e-03, -2.00990285e-03, -3.04437871e-03],\n",
       "        [-1.19657292e-04, -7.99915229e-04,  1.24271575e-03, ...,\n",
       "          4.77115158e-03, -2.07328075e-03, -3.21268733e-03],\n",
       "        [ 8.83071789e-06, -8.24735616e-04,  1.31704286e-03, ...,\n",
       "          4.95199347e-03, -2.11456860e-03, -3.33989342e-03]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model에 데이터 태우기\n",
    "\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-ecology",
   "metadata": {},
   "source": [
    "#### \"<tf.Tensor: shape=(256, 20, 7001)\"임을 알 수 있음.\n",
    "#### 7001은 Dense 레이어의 출력 차원수. 7001개 단어 중 어느 단어의 확률이 가장 높을지를 모델링해야하기 때문\n",
    "#### 256은 이전 스텝에서 지정한 배치 사이즈\n",
    "#### dataset.take(1)를 통해 1개의 배치, 즉 256개의 문장을 가져온 것\n",
    "#### 20은 tf.keras.layers.LSTM(hidden_size, return_sequences=True)로 호출한 LSTM 레이어에서 return_sequences=True라고 지정한 부분에 있음\n",
    "#### return_sequences=True 뜻 : 즉, LSTM은 자신에게 입력된 시퀀스의 길이만큼 동일한 길이의 시퀀스를 출력한다는 의미\n",
    "#### return_sequences=False이면 LSTM 레이어는 1개의 벡터만 출력\n",
    "\n",
    "#### 문제는 우리의 모델은 입력 데이터의 시퀀스 길이가 얼마인지 모름\n",
    "#### 그럼 20은 언제 알게 된 것일까? -> 데이터를 입력받으면서 알게됨. 우리 데이터셋의 max_len이 20으로 맞춰져 있던 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "weekly-aquarium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      multiple                  1792256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  7176025   \n",
      "=================================================================\n",
      "Total params: 22,607,961\n",
      "Trainable params: 22,607,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dress-centre",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()호출 가능\n",
    "# 그러나 output shape가 없음\n",
    "# 왜냐하면 우리의 모델은 입력 시퀀스의 길이를 모르기때문에 output shape를 특정할 수 없음\n",
    "# 하지만 모델의 파라미터 사이즈는 측정 가능. 약 2천2백만"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "strange-quality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "93/93 [==============================] - 12s 132ms/step - loss: 3.4619\n",
      "Epoch 2/30\n",
      "93/93 [==============================] - 12s 132ms/step - loss: 2.7997\n",
      "Epoch 3/30\n",
      "93/93 [==============================] - 13s 134ms/step - loss: 2.7038\n",
      "Epoch 4/30\n",
      "93/93 [==============================] - 12s 133ms/step - loss: 2.6135\n",
      "Epoch 5/30\n",
      "93/93 [==============================] - 12s 134ms/step - loss: 2.5469\n",
      "Epoch 6/30\n",
      "93/93 [==============================] - 13s 136ms/step - loss: 2.4970\n",
      "Epoch 7/30\n",
      "93/93 [==============================] - 13s 136ms/step - loss: 2.4437\n",
      "Epoch 8/30\n",
      "93/93 [==============================] - 13s 135ms/step - loss: 2.3972\n",
      "Epoch 9/30\n",
      "93/93 [==============================] - 13s 136ms/step - loss: 2.3550\n",
      "Epoch 10/30\n",
      "93/93 [==============================] - 13s 136ms/step - loss: 2.3146\n",
      "Epoch 11/30\n",
      "93/93 [==============================] - 13s 137ms/step - loss: 2.2741\n",
      "Epoch 12/30\n",
      "93/93 [==============================] - 13s 136ms/step - loss: 2.2357\n",
      "Epoch 13/30\n",
      "93/93 [==============================] - 13s 137ms/step - loss: 2.1955\n",
      "Epoch 14/30\n",
      "93/93 [==============================] - 13s 136ms/step - loss: 2.1547\n",
      "Epoch 15/30\n",
      "93/93 [==============================] - 13s 136ms/step - loss: 2.1157\n",
      "Epoch 16/30\n",
      "93/93 [==============================] - 13s 136ms/step - loss: 2.0785\n",
      "Epoch 17/30\n",
      "93/93 [==============================] - 13s 137ms/step - loss: 2.0385\n",
      "Epoch 18/30\n",
      "93/93 [==============================] - 13s 139ms/step - loss: 1.9991\n",
      "Epoch 19/30\n",
      "93/93 [==============================] - 13s 138ms/step - loss: 1.9605\n",
      "Epoch 20/30\n",
      "93/93 [==============================] - 13s 137ms/step - loss: 1.9231\n",
      "Epoch 21/30\n",
      "93/93 [==============================] - 13s 136ms/step - loss: 1.8865\n",
      "Epoch 22/30\n",
      "93/93 [==============================] - 13s 136ms/step - loss: 1.8475\n",
      "Epoch 23/30\n",
      "93/93 [==============================] - 13s 136ms/step - loss: 1.8107\n",
      "Epoch 24/30\n",
      "93/93 [==============================] - 13s 136ms/step - loss: 1.7725\n",
      "Epoch 25/30\n",
      "93/93 [==============================] - 13s 136ms/step - loss: 1.7361\n",
      "Epoch 26/30\n",
      "93/93 [==============================] - 13s 137ms/step - loss: 1.6993\n",
      "Epoch 27/30\n",
      "93/93 [==============================] - 13s 137ms/step - loss: 1.6642\n",
      "Epoch 28/30\n",
      "93/93 [==============================] - 13s 139ms/step - loss: 1.6286\n",
      "Epoch 29/30\n",
      "93/93 [==============================] - 13s 137ms/step - loss: 1.5921\n",
      "Epoch 30/30\n",
      "93/93 [==============================] - 13s 138ms/step - loss: 1.5554\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fade8618610>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 학습 시작\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-patch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss는 모델이 오답을 만들고 있는 정도로 해석하면 됨\n",
    "# 그렇다고 loss가 1일때 99%를 맞추고 잇는 것은 아님\n",
    "# 오답율이 감소하고 있으니 학습이 잘 진행되고 있다 정도로만 해석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-subsection",
   "metadata": {},
   "source": [
    "# 6-6. 실습 (3) 잘 만들어졌는지 평가하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "stylish-nature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "# generate_text 함수는 모델에게 시작 문장을 전달하면 모델이 시작 문장을 바탕으로 작문을 진행함\n",
    "\n",
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해 입력받은 init_sentence도 일단 텐서로 변환\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "    \n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성 해야함\n",
    "    while True:\n",
    "        predict = model(test_tensor) # 입력받은 문장의 텐서를 입력\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] # 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됨\n",
    "        \n",
    "        # 모델이 새롭게 예측한 단어를 입력 문장 뒤에 붙여줌\n",
    "        test_tensor = tf.concat([test_tensor, \n",
    "                                 tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        \n",
    "        # 모델이 <end>를 예측했거나, max_len에 도달하지 않았으면 while루프를 또 돌면서 다음 단어를 예측\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "            \n",
    "    generated = \"\"\n",
    "              \n",
    "    # 생성된 tensor안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "        \n",
    "    return generated # 이것이 최종적으로 모델이 생성한 자연어 문장\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secure-contribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습단계에서는 while문이 필요 없었음. \n",
    "# 소스문장과 타겟문장이있고 우리는 소스문장을 모델에 입력해서 나온 결과를 타겟문장과 비교하기만 하면 됐음\n",
    "# 그러나 텍스트를 실제로 생성해야하는 시점에서 우리는 2가지가 없음 1. 타겟문장, 2. 소스문장\n",
    "# 우리는 텍스트 생성 태스크를 위해 테스트 데이터셋을 따로 생성한 적이 없음\n",
    "\n",
    "# generat_text()함수에서 init_sentence를 인자로 받고 있음\n",
    "# 이렇게 받은 인자를 일단 텐서로 만들고 디폴트로는 <start> 단어 하나만 받음\n",
    "\n",
    "# 1. while의 첫번째 루프에서 test_tensor에 <start> 들어갔다고 가정, 우리의 모델이 출력으로 7001개의 단어 중 A를 골랐다고 하면\n",
    "# 2. while의 두번째 루프에서 test_tensor에는 <start> A가 들어감. 그래서 우리의 모델이 그 다음 B를 골랐다고 하면\n",
    "# 3. while의 세번째 루프에서 test_tensor에는 <start> A B가 들어감 그래서 ....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "instant-island",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> home to the king s ship , sir , that you have been so <end> '"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 위 문장 생성 함수 실행\n",
    "generate_text(model, tokenizer, init_sentence=\"<start> home\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hundred-murder",
   "metadata": {},
   "source": [
    "# 6-7. 프로젝트: 멋진 작사가 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "printable-glance",
   "metadata": {},
   "source": [
    "## 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entertaining-tulsa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 준비\n",
    "# wget https://aiffelstaticprd.blob.core.windows.net/media/documents/song_lyrics.zip\n",
    "# unzip song_lyrics.zip -d ~/aiffel/lyricist/data/lyrics  #lyrics 폴더에 압축풀기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-weapon",
   "metadata": {},
   "source": [
    "## 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "persistent-basket",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기:  187088\n",
      "Examples:\n",
      " [\"you're driving me insane\", 'Crazy, baby', 'Oh, girl', \"Crazy 'bout your love\", '', 'Soft and wet (U know)', 'You are soft and wet (Oh, sugar)', 'Your love is soft and wet', 'Soft and wet', '', '', 'You have got a strange way about you', \"Kinda' crazy but I love you just the same\", 'Because you, oh you make me wanna do, oh everything', '', \"I'm just a crazy fool, lost in the world of love\", \"I get from crazy you, oh, I'm so strung out\", \"Thinking 'bout the crazy things you do, crazy you\", '', '', \"You can live your own life and I'll live mine\", 'I will never try to keep you down', 'And even if I only see you some of the time', \"I'm just happy when you come around\", \"And even if the sun don't shine\", '', \"I'm warm enough when\", \"You're in these arms of mine\", \"Just as long as we're together\", \"Every thing's alright, every thing's alright\", '', 'There is nothing that will overcome the love we share', 'Nothing that will break us apart', 'Girl, I gotta always have you in my ear', 'Gotta always have you in my heart', 'Oh baby, your place or mine?', \"I'll get the music, baby, you bring the wine\", '', \"Just as long as we're together\", \"Oh girl, there's nothing better\", \"Just as long as we're together\", \"Every thing's alright, every thing's alright\", '', 'Just as long as there is you', \"I'll be around doing what you what me to\", 'Just as long as there is me', \"I'll be around to sing that melody, sugar\", '', \"Just as long as we're together\", \"Oh baby, every thing's alright\", \"Just as long as we're together\", \"Oh girl, every thing's alright\", \"Just as long as we're together\", \"Oh girl, every thing's alright\", '', \"Don't you know there ain't better oh\", '', '', 'Baby, what are we gonna do?', \"I'm so in love with you\", 'Baby, what are we gonna do?', \"I know you're in love with me too\", '', \"Should we go on livin' together\", 'Or should we get married right away?', \"Whatever you decide, I'll still love you, baby\", \"An' we'll grow stronger everyday\", '', 'Baby, what are we gonna do?', 'I barely have enough money for two', 'Baby, what are we gonna do?', \"I don't want to regret what I've done to you\", '', \"I never would've thought that this would happen\", 'To a very careful man like me', \"But baby, pretty baby, we're gonna work it out, yeah\", \"I love you, I love you, can't you see?\", '', 'Baby, baby, baby', \"It's you, said it's you, I truly adore\", \"Baby, baby, we're gonna work it out\", 'I hope our baby has eyes just like yours', '', '', \"You're always on my mind\", 'Day and night, baby, all the time', '(All the time)', 'You mean so much to me', 'A love like ours just hard to be', '', \"You're the wind and the rain\", \"You've got a river that takes away my pain\", \"And the sky that's, oh, so blue\", \"You're everything girl, don't you know I need you\", '(Need you)', '', \"You are the only thing that keeps me goin'\", 'You are the only thing that keeps my love alive', \"You are the only thing that I'll ever need\", \"You don't have to worry, you see, my love is forever\", '', \"Sugar, I don't have to dream\", \"'Cause you give me what I want\", 'Girl, you know what I need', 'Forever until my love is through, I, I will love you', '(Love you)', '', 'Oh, oh, what does it take to make you see', 'That you are the only one for me', \"I'll never, never stop loving you\", 'Never, never stop wanting you', 'Never, never get enough of you, love is forever', '', \"You are the only thing that keeps me goin'\", 'You are the only thing, keeps my love alive', \"You are the only thing that I'll ever need\", \"You don't have to worry, my love is forever\", '', '', 'I had everything I needed', 'But now my life is so blue', 'You meant the world to me', \"But now you're gone and I'm so blue\", '', 'Even though the sun is shining', 'I spend my day, I spend my day crying over you', 'Oh yeah, I spend my day crying over you', 'I feel just like the sky, oh, so blue', '', \"Oh baby, don't you know?\", \"Baby don't you know?\", \"I spend my nights, it's all alone\", 'Talking to myself, I am so blue', '', 'And everyday the feeling gets stronger', \"And who's to say, just how much longer\", 'I can spend my nights all alone', '', 'Talking to myself and just crying over you', \"I guess I'll just spend my life\", 'So, so blue', '', '', 'Yeah, yeah, yeah, yeah!', 'Up until the other day', 'There remained an empty space within my bed', 'Then I took one look at you', 'And naughty things that we could do', '', 'I took one look at you', 'And all the things that we could do', 'Dance within my head', '', 'Never have I ever made love before', 'Never have I wanted to till now', \"Lover, can't you see I want you more and more\", \"Take me baby, yea, I'm yours\", '', \"I'll give you what you want\", 'But please love me now', \"I'll do anything you want\", \"You're the teacher, show me how\", '', 'Never have I ever made love before', 'Never have I wanted to till now', \"Lover, can't you see I want you more and more\", 'Take me baby, yea', \"I'm yours\", '', '', \"I ain't got no money\", \"I ain't like those other guys you hang around\", \"And it's kinda funny\", 'But they always seem to let you down', 'And I get discouraged', \"'cause I never see you anymore\", 'And I need your love, babe yeah', \"That's all I'm living for, yeah\", \"I didn't wanna pressure you, baby\", 'But all I ever wanted to do', 'I wanna be your lover', 'I wanna be the only one that makes you come running', 'I wanna be your lover', 'I wanna turn you on, turn you out, all night long, make you shout', 'Oh, lover! Yeah', 'I wanna be the only one you come for', '', 'I wanna be your brother', 'I wanna be your mother and your sister, too', \"There ain't no other\", \"That can do the things that I'll do to you\", 'And I get discouraged', \"'cause you treat me just like a child\", \"And they say I'm so shy, yeah\", 'But with you I just go wild!', \"I didn't wanna pressure you, baby. No!\", 'But all I ever wanted to do', '', 'I wanna be your lover', 'I wanna be the only one that makes you come running', 'I wanna be your lover', 'I wanna turn you on, turn you out, all night long make you shout', 'Oh, lover! Yeah', 'I wanna be the only one you come for, yeah', '', '', \"There's some talk going 'round town\", \"That you really don't give a damn\", 'They say you really put me down', \"When I'm doing the best I can\", 'I gave you all of my love', 'I even gave you my body', \"Tell me, baby, ain't that enough?\", 'What more do you want me to do?', \"I play the fool when we're together\", \"But I cry when we're apart, yeah\", \"I couldn't do you no better\", \"Don't break what left of my broken heart, baby\", '', 'Why you wanna treat me so bad', 'When you know I love you?', 'How can you do this to me', 'When you know I care?', 'Why you wanna treat me so bad', 'When you know I love you?', '', 'You know, I try so hard', 'To keep you satisfied', 'Sometimes you play the part', \"Sometimes you're so full of pride\", \"And if it's still good to ya\", 'Why you wanna treat me so bad?', \"You used to love it when I'd do you\", \"You used to say I was the best you'd ever had\", '', \"I play the fool when we're together\", 'I give you everything I can, yeah', \"And if it's still good to ya\", \"There's something that I can't understand\", '', 'Why you wanna treat me so bad', 'When you know I love you?', 'How can you do this to me', 'When you know I care?', 'Why you wanna treat me so bad', 'When you know I love you?', '', '', 'Sexy dancer', 'Sexy dancer', \"Sexy dancer, you got my body screamin'\", \"Sexy dancer, you got me just-a-creamin'\", 'Sexy dancer, when you rub my body', 'Sexy dancer, it gets me so hot...hot!', 'Sexy dancer, sexy dancer', 'Sexy dancer, sexy dancer', '', 'Sexy dancer, I want your body, want your body', 'Sexy dancer, dance', 'Sexy dancer, I want your body, want your body', 'Sexy dancer, dance', 'Sexy dancer, I want your body, want your body', 'Sexy dancer, dance', 'Sexy dancer, I want your body, want your body', 'Sexy dancer, dance', '', 'Sexy dancer, do that sexy dance', 'Sexy dancer', '', 'Sexy dancer, sexy dancer', 'Sexy dancer, sexy dancer', 'Hot!', '', '', \"When we're dancing close and slow\", 'I never want to let you go, no, no', 'I feel your warm embrace', 'The softness of your face', 'Tell me, baby, are we here alone?', \"When we're kissing long and hard\", 'I can almost taste the thoughts within your mind', 'Sex-related fantasy is all that my mind can see', \"Baby, that's honestly the way I feel\", '', \"When we're dancing close and slow\", \"I'm not afraid to let my feelings show\", 'I want to come inside of you', \"I want to hold you when we're through\", \"Can't you feel my love touching you?\", '', '', \"I've held your hand so many times\", 'But I still get the feeling I felt the very first time', \"I've kissed your lips and laid with you\", \"And I cherish every moment we spend in each other's arms\", 'I guess my eyes can only see as far as you', 'I only want to be with you', '', \"We've come so far in so little time\", 'Sometimes I wonder if this is meant to be', 'Sometimes you are so very kind', \"That the nights you're not with me I'm scared that you're gonna leave\", '', \"I guess you could say that I'm just being a fool\", 'But I only want to be with you', '', \"I guess you could say that I'm just being a fool\", 'But I always, always want to be with you', '', '', 'I knew from the start', 'That I loved you with all my heart', 'But you were untrue.', 'You had another lover and she looked just like you', \"Bambi, can't you understand?\", \"Bambi, it's better with a man\", '', \"It's so hard to believe\", \"Maybe it's because you're so young\", \"Or maybe I'm just too naive\", \"Who's to say, maybe you're really having fun\", '', \"Bambi, can't you understand?\", \"Bambi, it's better with a man\", '', 'All your lovers - they look just like you', 'But they can only do the things that you do', 'Come on, baby, and take me by the hand', \"I'm gonna show what it's like to be loved by a man\", '', 'Bambi, I know what you need', 'Bambi, maybe you need to bleed', '', 'Yeah!', '', '', 'All my friends tell me', \"About the loves they've had\", \"Can't they see what they're doing to me?\", 'It makes me feel so bad', \"'Cause I'm so alone\", 'And brokenhearted', \"It ain't like my life is ended\", 'But more like it never started', 'The love my friends rap about I keep anticipating', \"I try so hard but don't you know, my patience is fading away\", 'Still waiting', \"I'm waiting for that love\", 'Still waiting', 'I wish on every star above', 'Still waiting', 'Waiting for the love to come around', 'Oh, love.', 'Waiting for the love to come around', '', \"People say that I'm too young\", 'Too young to fall in love', \"But they don't know, they really don't know\", \"That's all that I've been dreaming of\", \"'Cause I spend my nights just a-crying\", 'And I spend my days just a-trying', 'To find that love to call my own', \"'Cause I'm sick and tired of being alone\", '', 'Still waiting', 'Waiting for that love', 'Still waiting', 'I wish on every star above', 'Still waiting', \"I'm waiting for the love to come around.\", \"(Waiting for the love) Don't you know that I'm waiting\", '(Waiting for the love) Say..', '(Waiting for the love) To come around now', '', 'I need somebody to hold on to baby', \"Waiting and waiting, don't you know that I'm\", '', 'Still waiting', \"I'm waiting for the love, sugar\", 'Still waiting', \"If you're out there girl, please come to me\", 'Still waiting', \"Don't make me cry no more\", 'Still waiting', \"If you're out there, baby, please come to me\", 'Still waiting', 'Wishing and wishing for days, baby', 'Still waiting', \"If you're out there, girl, please come to me\", 'Still waiting', '', '', 'Baby, baby, when I look at you', 'I get a warm feeling inside', \"There's something about the things you do\", 'That keeps me satisfied', \"I wouldn't lie to you, baby\", \"It's mainly a physical thing\", 'This feeling that I got for you, baby', 'It makes me wanna sing', 'I feel for you - I think I love you', 'I feel for you - I think I love you', '', \"Baby, baby, when I lay wit' you\", \"There's no place I'd rather be\", \"I can't believe, can't believe it's true\", 'The things that you do to me', \"I wouldn't lie to you, baby\", \"I'm physically attracted to you\", 'This feeling that I got for you, baby (Ooh baby)', \"There's nothing that I wouldn't do (for you girl)\", '', 'I feel for you - I think I love you', 'I feel for you - I think I love you', '', 'Play', '', \"I think it's love\", \"I feel for you - I think it's love\", 'I feel for you - I think I love you', \"I feel for you - I think it's love\", '', 'Play', '', 'I feel for you - I think I love you', 'I feel for you - I think I love you', '', '', \"We've been together for quite some time\", \"I'd think by now you'd know\", 'It would take forever to get you off my mind', 'If ever you decide to go', \"I guess I got a little insecurity when it's concerning you\", \"I guess I'm just afraid that if you ever leave\", \"I'd be in a messed-up state of blue\", \"And I'd be so lonely\", '', 'Without you loving me', \"I know it's gonna be lonely\", 'Without you giving me everylittlesinglething that I need, lonely', '', \"Whatever's in your kiss, it really turns me on\", \"'til I go right out of my mind\", 'And who could ever resist your accent from gay Paree', 'It gets me every time', 'I betcha thatcha never knew that in my dreams you are the star', 'The only bummer is that you always want to leave', 'Who do you think you are', '', \"Don't you know it's gonna be lonely\", 'Without you loving me', \"I know it's gonna be lonely\", 'Without you giving me everylittlesinglething that I need, lonely', '', \"It's gonna be lonely\", '', 'Without you loving me', 'Giving me everything that I need', \"Oh pretty baby, can't you see\", \"It's gonna be lonely\", '', 'Without you loving me', \"I know, I know it's gonna be lonely\", \"Oh, whatever's in your kiss\", 'I never could resist', \"Oh, baby don't go!\", '', \"We've been together for quite some time\", \"It'd take forever to get you off my mind\", 'Oh, girl!', '', 'Without you loving me', \"I know it's gonna be lonely\", 'Without you by my side', \"Don't you know that I could die, baby\", '', 'Without you loving me', \"Can't you see\", \"It's gonna be lonely\", '', \"It's gonna be lonely, baby\", 'So lonely, baby', '', '', \"There's something about u, baby\", 'It happens all the time', \"Whenever I'm around u, baby\", 'I get a dirty mind', \"It doesn't matter where we are\", \"It doesn't matter who's around\", \"It doesn't matter\", 'I just wanna lay ya down', \"In my daddy's car\", \"It's you I really wanna drive\", 'But you never go too far', 'I may not be your kind of man', 'I may not be your style', 'But honey all I wanna do', 'Is just love you for a little while', '', 'If you got the time', \"I'll give you some money\", 'To buy a dirty mind', \"Don't misunderstand me\", 'I never fool around', 'But honey you got me on my knees', \"Won't you please let me lay ya down (down, down, down...)\", '', 'I really get a dirty mind (mind, mind, mind...)', \"Whenever you're around\", 'It happens to me everytime (time, time, time...)', '', 'You just gotta let me lay ya', 'Gotta let me lay ya, lay ya', 'You just gotta let me lay ya', 'Gotta let me lay ya down', '', \"In my daddy's car\", \"It's you I really wanna drive\", 'Underneath the stars', 'I really get a dirty mind', \"Whenever you're around\", '', \"I don't wanna hurt you, baby\", 'I only want to lay you down', '', '', 'When you were mine', 'I gave you all of my money', 'Time after time', 'You done me wrong', 'It was just like a dream', 'You let all my friends come over and meet', 'And you were so strange', \"You didn't have the decency \", 'To change the sheets', '', 'Oh girl, when you were mine', 'I used to let you wear all my clothes', 'You were so fine (so fine)', \"Maybe that's the reason\", 'That it hurt me so', '', 'I know (I know)', \"That you're going with another guy\", \"I don't care (don't care)\", \"Cuz I love u, baby, that's no lie\", 'I love you more than I did', 'When you were mine', '', 'When you were mine', 'You were kinda sorta my best friend', 'So I was blind (so blind)', 'I let you fool around', \"I never cared (didn't care)\", 'I never was the kind to make a fuss', 'When he was there', 'Sleeping inbetween the two of us', '', 'I know (I know)', \"That you're going with another guy\", \"I don't care (don't care)\", \"Cuz I love you, baby, that's no lie\", 'I love you more than I did', 'When you were mine', '', 'When you were mine', 'U were all I ever wanted to do', 'Now I spend my time', \"Following him whenever he's with you\", '', 'I know (I know)', \"That you're going with another guy\", \"I don't care (don't care)\", \"Cuz I love you, baby, that's no lie\", 'I love you more that I did', 'When you were mine', '', 'When you were mine, yeah, oh no', 'Love you, baby', 'Love you, baby', 'When you were mine', '', '', 'Pardon me, I wanna talk to you', 'I may be kinda shy But I just gotta tell you', \"What I'm going to do\", 'Someone over there says He wants to get to know you', \"I don't care cuz\", 'I really wanna hold you', \"And I'm so scared\", '[Girl], he might do something', 'To you that you like', \"Now I've been waiting\", 'Such a bloody long time', 'Just to get this close to you', \"Now that you're near me\", 'I want you to hear me', \"I'll tell you what I wanna do\", 'Oh, I wanna do it', 'Do it all night', 'I wanna do it', 'Do it to you right', 'Giving up so easy', 'Is something that I never do', \"But I'm so easy, so easy\", \"When it comes to loving you Can't you understand that I want\", 'To hug and kiss you', \"I'll do anything I can just\", 'To give you happiness', 'And I drown, baby, drown, baby', \"In your arms, c'mon baby\", \"Can't you get to this?\", \"I've been waiting such a bloody long time\", \"And you're talking to someone else\", \"Now that I've got your attention\", \"There's something I wanna mention\", '', 'I wanna do it', 'Do it all night', 'I wanna do it, oh yeah', 'Do it to you right', 'Do it all night', 'I,I wanna do it, oh yeah', 'Do it to you right', '', 'I wanna do it', 'Do it all night', 'I wanna do it, oh yeah', 'Do it to you right', '', 'gotta do it, do it', 'Do it all night', 'Do it to you right', '', 'Do it all night', 'Do it to you right', '', '', \"I've gotta broken heart again\", \"Cuz we're only supposed to be friends\", 'You see he stole my old lady away from me', \"And now I'm just as blue as I can be\", \"I've gotta broken heart again\", \"Cuz I ain't got no money to spend\", 'You see I spend it all on long distance phone calls', 'Beggin her to please come home, yeah, yeah', 'Ah, yeah', \"It doesn't matter what I do\", \"I can't stop, ah, thinking about uou\", 'The little things you said', 'The things you do to me in bed', \"Oh baby, I can't get you outta my head\", 'Oh, oh, gotta broken heart again, yeah', \"This time it's serious\", 'It feels just like the end', 'Cuz once your love has gone away', \"There ain't nothing, nothing left to say\", '', '', 'She saw me walking down the streets', 'Of your fine city', 'It kinda turned me on when she looked at me', 'And said, \"C\\'mere\"', \"Now I don't usually talk to strangers\", 'But she looked so pretty', 'What can I lose,', 'If I, uh, just give her a little ear?', '\"What\\'s up little girl?\"', '\"I ain\\'t got time to play.\"', \"Baby didn't say too much\", 'She said, \"Are you gay?\"', 'Kinda took me by suprise', \"I didn't know what to do\", 'I just looked her in her eyes', 'And I said, \"No, are u?\"', 'Said to myself, said', '\"She\\'s just a crazy, crazy, crazy', 'Little mixed up dame.', \"She's just a victim of society\", 'And all it\\'s games.\"', 'Now where I come from', \"We don't let society\", \"Tell us how it's supposed to be\", 'Our clothes, our hair', \"We don't care\", \"It's all about being there\", \"Everybody's going Uptown\", \"That's where I wanna be\", 'Uptown', 'Set your mind free', 'Uptown', 'Got my body hot', 'Get down', \"I don't wanna stop, no\", 'As soon as we got there', 'Good times were rolling', 'White, Black, Puerto Rican', \"Everybody just a-freakin'\", 'Good times were rolling', '', 'She started dancing in the streets', \"Ow, girl, she's just gone mad\", 'U know, she even made love to me', 'Ooh, best night I ever had', 'Ah yeah', 'I never talk to strangers', \"But this time it's all right\", 'See, she got me hot, ah', \"I couldn't stop, ah\", 'Good times were rolling all night All night, yeah', 'Now where I come from', \"We don't give a damn\", 'We do whatever we please', \"It ain't about no downtown\", 'Nowhere bound', 'Narrow-minded drag', \"It's all about being free\", \"Everybody's going Uptown\", \"It's where I wanna be\", 'Uptown', 'U can set your mind free, yeah', 'Uptown', 'Keep your body hot', 'Get down', \"I don't wanna stop, no\", 'Uptown', 'Ooh, ooh, yeah', 'Uptown', \"Everybody's going, everybody's going\", 'Everybody gotta gotta', 'Uptown', 'Now go-go-go, go-go-go', \"They're going\", 'Uptown', '', 'Uptown', 'Yeah, yeah, now, all now', 'Uptown', 'Gotta go-go-go', 'Uptown', 'Yeah, gotta go, gotta go', 'Uptown', \"C'mon, c'mon, you\", 'U have to, you gotta go', 'Uptown', 'oh yeah', 'YEAH!', '', '', 'I remember when I met u, baby', \"You were on you're way to be wed\", 'You were such a sexy thing', 'I loved the way you walked The things you said', 'And I was so non-chalant', \"I didn't want you to be mis-led\", \"But I've gotta have u, baby\", 'I got to have you in my bed', 'And you said', '\"But I just a virgin And I\\'m on my way to be wed', \"But you're such a hunk\", 'So full of spunk,', \"I'll give you Head\", \"Til you're burning up\", 'Head', 'Til you get enough', 'Head', \"Til you're love is red\", 'Head', \"Love you til you're dead\", \"You know you're good, girl\", 'I think you like to go down', \"You wouldn't have stopped\", 'But I, I came on your wedding gown', 'And you said, \"I must confess,', 'I wanna get undressed and go to bed.\"', 'With that I jammed, you fool,', 'You married me instead', 'Now morning, noon, and night', 'I give u', 'Head', \"Til you're burning up\", 'Head', 'Til you get enough', 'Head', \"Til you're love is red\", \"Head Love you til you're dead\", 'Ooh yeah, ooh yeah, ooh yeah', 'Head, Head, Head, oh, Head, oh', 'Head', \"Till you're burning up\", 'Head', 'Til you get enough', 'Head', 'Til your love is red', 'Head', \"Love you til you're dead\", 'Head, Head, Head', '(Ooh, baby, no)', 'Head, Head', 'Ooh, Head Head', '(You said I could)', 'Head', 'Head', '', '', \"I was only 16, but I guess that's no excuse\", \"My sister was burnin' to love me and loose\", '', \"She don't wear no underwear\", \"She's so lonely, gets in her hair\", \"And it's got a funny way of stoppin' the juice\", '', 'My sister never made love to anyone else but me', \"She's the reason for my, uh, sexuality\", '', \"Showed me where it's supposed to go\", \"A blow job doesn't mean blow\", \"Incest is everything it's said to be\", '', 'Oh, sister', \"Don't put me on the street again\", 'Oh, sister', 'I just want to be your friend', '', 'I was only 16 and only half a man', \"My sister didn't give a goddamn\", 'She only wanted to turn me out', '', 'She took a whip to me until I shout', \"Oh, motherfucker's, just a motherfucker\", \"Can't you understand?\", '', 'Oh, sister', \"Don't put me on the street again\", 'Oh, sister', 'I just want to be your friend', '', 'I know what you want me to do', 'Put me on the street and make me blue', 'Oh, sister, ooh sister, ooh', '', '', \"We don't give a damn\", 'We just wanna jam', 'Party up', '', 'That army bag', 'Such a double drag', 'Party up', '', 'Party, got to party down, baby', 'Revolutionary rock and roll', \"Goin' uptown, baby\", 'How you gonna make me kill somebody', \"I don't even know?\", '', 'They got the draft, uh uh', 'I just laugh', 'Party up', '', \"Fightin' war\", \"Is such a fuckin' bore\", 'Party up', '', 'Party, uh uh, got to party down, babe', \"Ooh, it's all about what's in your mind\", \"Goin' uptown, baby\", \"I don't wanna die\", 'I just wanna have a bloody good time', '', 'Party up', 'Got to party up', 'Party up', 'Got to party up', '', 'Because of their half-baked mistakes', 'We get ice cream, no cake', 'All lies, no truth', 'Is it fair to kill the youth?', '', 'Party up', 'Got to party up, yeah', 'Got to party up, babe, ooh', 'Got to party up, yeah', 'Got to party up, babe, ooh', '', 'Got to party up, yeah, ooh ooh', 'Got to party up, babe, ooh', 'Got to party up, yeah', 'Got to party up, babe, ooh', 'Yeah, yeah, yeah, yeah', '', 'Party up', 'Got to party up', 'Party up', 'Got to party up', '', 'Party up', 'Got to party up']\t"
     ]
    }
   ],
   "source": [
    "# 데이터 읽어오기\n",
    "# glob 모듈을 사용하면 파일 읽어오는 작업 하기 용이함\n",
    "# glob을 활용하여 모든 txt파일을 읽어온 후, raw_corpus 리스트에 문장 단위로 저장\n",
    "\n",
    "import glob\n",
    "import re       # 정규표현식을 위한 Regax 지원 모듈(문장 데이터 정돈에 필요)\n",
    "import numpy as np       # 변환된 문장 데이터(행렬)을 편하게 처리하기 위해\n",
    "import tensorflow as tf   # 텐서플로우\n",
    "import os\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러 개의 txt 파일을 모두 읽어서 raw_corpus에 담김\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "        \n",
    "print(\"데이터 크기: \", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[100:1000], end=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entertaining-sympathy",
   "metadata": {},
   "source": [
    "## 데이터 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-thomson",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 데이터 정제\n",
    "## preprocess_sentence() 함수 활용\n",
    "## 긴 문장은 과도한 padding을 갖게 하므로 제거.\n",
    "## 그래서 이번에는 문장을 토큰화 했을 때 토큰의 개수가 15개를 넘어가는 문장을 학습데이터에서 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "willing-affect",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (<ipython-input-244-7d00a9203009>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-244-7d00a9203009>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    if \\ in sentence : continue\u001b[0m\n\u001b[0m                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue # 공백 건너뜀\n",
    "    if sentence[0] == \"'\": continue # 문장 시작이 ' 건너뜀\n",
    "    if sentence[-1] == \"'\": continue # 문장 끝이 ' 건너뜀\n",
    "    if sentence[0] == '\"': continue # 문장 시작이 \" 건너뜀\n",
    "    if sentence[-1] == '\"': continue # 문장 끝이 \" 건너뜀\n",
    "    if sentence[0] == '(': continue # 문장 끝이 ( 건너뜀\n",
    "    if sentence[-1] == ')': continue # 문장 끝이 ) 건너뜀\n",
    "    if sentence[0] == '[': continue # 문장 끝이 [ 건너뜀\n",
    "    if sentence[-1] == ']': continue # 문장 끝이 ] 건너뜀        \n",
    "    \n",
    "    \n",
    "#     if idx >= 15: continue # 15개 넘어가는 문장 건너뜀 : <=15는 15이하여서 15개 이하 건너 뛰는 거 아닌가? \n",
    "    if idx >10: break # 일단 문장 10개에서 끝내\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "sealed-sigma",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "knowing-escape",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<start>', 'this', 'is', 'sample', 'sentence', '.', '<end>']\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 소문자로 바꾸고 양쪽 공백 삭제\n",
    "    \n",
    "    # 아래 3단계를 거쳐 sentence는 스페이스 1개를 delimeter(구분문자)로 하는 소문자 단어 시퀀스로 바뀝니다.\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)        # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)                  # 공백 패턴을 만나면 스페이스 1개로 치환\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)  # a-zA-Z?.!,¿ 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환\n",
    "    \n",
    "    sentence = sentence.strip()\n",
    "     \n",
    "    sentence = '<start> ' + sentence + ' <end>'      # 이전 스텝에서 본 것처럼 문장 앞뒤로 <start>와 <end>를 단어처럼 붙여 줍니다\n",
    "    sentence = list(sentence.split())\n",
    "    len(sentence) < 15\n",
    "    return sentence\n",
    "\n",
    "\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))   # 이 문장이 어떻게 필터링되는지 확인해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "generous-stupid",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-339-a13c0b498510>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-338-1bda2ac2d075>\u001b[0m in \u001b[0;36mpreprocess_sentence\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 소문자로 바꾸고 양쪽 공백 삭제\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# 아래 3단계를 거쳐 sentence는 스페이스 1개를 delimeter(구분문자)로 하는 소문자 단어 시퀀스로 바뀝니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"([?.!,¿])\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mr\" \\1 \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "result = list(preprocess_sentence(sentence.split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "sexual-sport",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split()\n",
    "# result = preprocess_sentense\n",
    "# ()\n",
    "# len(result.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "inclusive-project",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정제함수 활용하여 정제 데이터 구축\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue # 공백 건너뜀\n",
    "    if len(sentence) == '.': continue # . 건너뜀\n",
    "    if sentence[0] == \"'\": continue # 문장 시작이 ' 건너뜀\n",
    "    if sentence[-1] == \"'\": continue # 문장 끝이 ' 건너뜀\n",
    "    if sentence[0] == '\"': continue # 문장 시작이 \" 건너뜀\n",
    "    if sentence[-1] == '\"': continue # 문장 끝이 \" 건너뜀\n",
    "    if sentence[0] == '(': continue # 문장 끝이 ( 건너뜀\n",
    "    if sentence[-1] == ')': continue # 문장 끝이 ) 건너뜀\n",
    "    if sentence[0] == '[': continue # 문장 끝이 [ 건너뜀\n",
    "    if sentence[-1] == ']': continue # 문장 끝이 ] 건너뜀  \n",
    "    if sentence < 30: continue # 15개 넘어가는 문장 건너뜀 # 이건 글자가 30자 미만인거고 단어가 30개 미만이려면?\n",
    "        \n",
    "    corpus.append(preprocess_sentence(sentence)) # corpus에 추가해라 만든 정제함수(preprocess_sentece)에 sentence를 넣어서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "stuffed-prediction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "print(type(corpus[2]))\n",
    "print(len(corpus[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-horse",
   "metadata": {},
   "source": [
    "# 평가 데이터셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-error",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 평가 데이터셋 분리\n",
    "## tokenize() 함수로 데이터를 tensor로 변환한 후 \n",
    "## sklearn 모듈의 train_test_split()함수를 사용해 훈련데이터와 평가데이터 분리\n",
    "## 단어장의 크기는 12,000이상으로 설정, 총 데이터의 20%를 평가 데이터셋으로 사용\n",
    "## enc_train, enc_val, dec_train, dec_val = <코드 작성>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "hundred-executive",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2  25  19 ...   0   0   0]\n",
      " [  2  29  36 ...   0   0   0]\n",
      " [  2  13 103 ...   0   0   0]\n",
      " ...\n",
      " [  2   5  22 ...   0   0   0]\n",
      " [  2  25   6 ...   0   0   0]\n",
      " [  2  82   4 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7f29fbaf9710>\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):  \n",
    "    # 텐서플로우에서 제공하는 Tokenizer패키지 생성\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words = 12000, # 전체 단어 개수\n",
    "        filters=' ', # 별도로 전처리 로직을 추가할 수 있지만 이번에는 사용 안 함\n",
    "        oov_token=\"<unk>\" # out-of-vocabulary, 사전에 없었던 단어는 어떤 토큰으로 대체할지\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus) # 우리가 구축한 corpus로부터 Tokenize가 사전을 자동 구축\n",
    "\n",
    "    # 이후 tokenizer를 활용하여 모델에 입력할 데이터셋 구축\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)  # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환\n",
    "    \n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기 위한 padding 메소드 제공\n",
    "    # maxlen의 디폴트 값은 None. 이 경우 corpus의 가장 긴 문장을 기준으로 시퀀스 길이가 맞춰짐\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post') # 토큰화 15개 미만?\n",
    "    \n",
    "    print(tensor, tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)\n",
    "\n",
    "print('ok')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "continental-reservoir",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26956"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.index_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "decimal-carbon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "<class 'keras_preprocessing.text.Tokenizer'>\n"
     ]
    }
   ],
   "source": [
    "print(type(tensor.shape))\n",
    "print(type(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "choice-symbol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    2    29    36     4 10949     8  3804   280     3     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "# 생성된 텐서 데이터를 출력\n",
    "print(tensor[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "foster-height",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skelarn의 train_test_split 함수를 통해 train_data와 test_data분리  \n",
    "# test_size 인자를 조절해주면 설정해준 값 만큼 test dataset 비율 조정 가능\n",
    "# from sklearn.datasets import corpus\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(tensor, tensor, test_size=0.2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "southwest-string",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (129334, 347)\n",
      "Target Train: (129334, 347)\n",
      "(32334, 347)\n",
      "(32334, 347)\n"
     ]
    }
   ],
   "source": [
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)\n",
    "print(enc_val.shape)\n",
    "print(dec_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "municipal-shield",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 위 과정까지 올바르게 진행했을 경우 아래의 실행결과 확인 가능\n",
    "# print(\"Source Train : \", enc_train.shape)\n",
    "# print(\"Target Train : \", dec_train.shape)\n",
    "\n",
    "# out:\n",
    "# Source Train: (124960, 14)\n",
    "# Target Train: (124960, 14)\n",
    "\n",
    "## 만약 결과가 다르면 동일한 결과를 얻도록 해야함\n",
    "## 학습데이터 개수가 124960 보다 크다면 step3다시 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-celebration",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-works",
   "metadata": {},
   "source": [
    "# Step 5. 인공지능 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insured-thunder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인공지능 만들기\n",
    "# 모델의 embedding size와 hideen size를 조절하여 10epoch안에 val_loss값을 2.2 수준으로 줄일 수 잇는 모델 설계\n",
    "# loss는 아래 제시된 loss함수 그대로 사용\n",
    "\n",
    "# loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "#     from_logits=True, reduction='none')\n",
    "\n",
    "# generate_text(lyricist, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-uncertainty",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-legend",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-penguin",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "informed-walter",
   "metadata": {},
   "source": [
    "# 공백으로 나눠진 문자열에서 단어가 몇개있는지 헤아리기!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
