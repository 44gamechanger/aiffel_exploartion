{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "essential-treatment",
   "metadata": {},
   "source": [
    "# 6. 작사가 인공지능 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binding-grove",
   "metadata": {},
   "source": [
    "# 6-7. 프로젝트: 멋진 작사가 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baking-latter",
   "metadata": {},
   "source": [
    "## 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "warming-briefs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 준비\n",
    "# wget https://aiffelstaticprd.blob.core.windows.net/media/documents/song_lyrics.zip\n",
    "# unzip song_lyrics.zip -d ~/aiffel/lyricist/data/lyrics  #lyrics 폴더에 압축풀기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affiliated-portugal",
   "metadata": {},
   "source": [
    "## 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "careful-minneapolis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "데이터 크기:  187088\n",
      "Examples:\n",
      " ['', '', 'All of this and more is for you', 'With love, sincerity and deepest care', 'My life with you I share', '', '', 'Ever since I met you, baby', \"I've been wantin' to lay you down\", \"But it's so hard to get you\"]\t"
     ]
    }
   ],
   "source": [
    "# 데이터 읽어오기\n",
    "# glob 모듈을 사용하면 파일 읽어오는 작업 하기 용이함\n",
    "# glob을 활용하여 모든 txt파일을 읽어온 후, raw_corpus 리스트에 문장 단위로 저장\n",
    "\n",
    "import glob\n",
    "import re       # 정규표현식을 위한 Regax 지원 모듈(문장 데이터 정돈에 필요)\n",
    "import numpy as np       # 변환된 문장 데이터(행렬)을 편하게 처리하기 위해\n",
    "import tensorflow as tf   # 텐서플로우\n",
    "import os\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러 개의 txt 파일을 모두 읽어서 raw_corpus에 담김\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "        \n",
    "print(type(raw_corpus))        \n",
    "print(\"데이터 크기: \", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:10], end=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-question",
   "metadata": {},
   "source": [
    "## 데이터 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "covered-master",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 데이터 정제\n",
    "## preprocess_sentence() 함수 활용\n",
    "## 긴 문장은 과도한 padding을 갖게 하므로 제거.\n",
    "## 그래서 이번에는 문장을 토큰화 했을 때 토큰의 개수가 15개를 넘어가는 문장을 학습데이터에서 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "advanced-timeline",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All of this and more is for you\n",
      "With love, sincerity and deepest care\n",
      "My life with you I share\n",
      "Ever since I met you, baby\n",
      "I've been wantin' to lay you down\n",
      "But it's so hard to get you\n",
      "Baby, when you never come around\n",
      "Every day that you keep it away\n",
      "It only makes me want it more\n",
      "Ooh baby, just say the word\n",
      "And I'll be at your door\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "#     if len(sentence)\n",
    "    if len(sentence) == 0: continue # 공백 건너뜀\n",
    "    if sentence[-1] == \":\": continue # 문장 시작이 ' 건너뜀\n",
    "#     if sentence[-1] == \"'\": continue # 문장 끝이 ' 건너뜀\n",
    "#     if sentence[0] == '\"': continue # 문장 시작이 \" 건너뜀\n",
    "#     if sentence[-1] == '\"': continue # 문장 끝이 \" 건너뜀\n",
    "#     if sentence[0] == '(': continue # 문장 끝이 ( 건너뜀\n",
    "#     if sentence[-1] == ')': continue # 문장 끝이 ) 건너뜀\n",
    "#     if sentence[0] == '[': continue # 문장 끝이 [ 건너뜀\n",
    "#     if sentence[-1] == ']': continue # 문장 끝이 ] 건너뜀        \n",
    "#     if len(sentence) <= 15 : continue\n",
    "    \n",
    "#     if idx >= 15: continue # 15개 넘어가는 문장 건너뜀 : <=15는 15이하여서 15개 이하 건너 뛰는 거 아닌가? \n",
    "    if idx >15: break # 일단 문장 10개에서 끝내\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "endangered-andrews",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "violent-invite",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split()\n",
    "# result = preprocess_sentense\n",
    "# ()\n",
    "# len(result.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "furnished-hurricane",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()       # 소문자로 바꾸고 양쪽 공백을 삭제\n",
    "  \n",
    "    # 아래 3단계를 거쳐 sentence는 스페이스 1개를 delimeter(구분문자)로 하는 소문자 단어 시퀀스로 바뀝니다.\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)        # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)                  # 공백 패턴을 만나면 스페이스 1개로 치환\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)  # a-zA-Z?.!,¿ 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "\n",
    "    \n",
    "    sentence = '<start> ' + sentence + ' <end>'      # 이전 스텝에서 본 것처럼 문장 앞뒤로 <start>와 <end>를 단어처럼 붙여 줍니다\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"This @_is ;;;sample    sentence.\"))   # 이 문장이 어떻게 필터링되는지 확인해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "indirect-faculty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_sentence_2(preprocess_sentence):\n",
    "#     for result in preprocess_sentence(sentence):\n",
    "#         if len(preprocess_sentence(sentence)) > 30: continue\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "frank-martin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(preprocess_sentence(sentence))\n",
    "# print(preprocess_sentence_2(preprocess_sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fabulous-integration",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(type(preprocess_sentence(sentence)))\n",
    "# print(len(preprocess_sentence(sentence)[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "appropriate-pathology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split()\n",
    "# result = preprocess_sentense\n",
    "# ()\n",
    "# len(result.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "caring-green",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정제함수 활용하여 정제 데이터 구축\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue # 공백 건너뜀\n",
    "    if sentence[-1] == \":\": continue # 문장 시작이 ' 건너뜀\n",
    "#     if sentence[-1] == \"'\": continue # 문장 끝이 ' 건너뜀\n",
    "#     if sentence[0] == '\"': continue # 문장 시작이 \" 건너뜀\n",
    "#     if sentence[-1] == '\"': continue # 문장 끝이 \" 건너뜀\n",
    "#     if sentence[0] == '(': continue # 문장 끝이 ( 건너뜀\n",
    "#     if sentence[-1] == ')': continue # 문장 끝이 ) 건너뜀\n",
    "#     if sentence[0] == '[': continue # 문장 끝이 [ 건너뜀\n",
    "#     if sentence[-1] == ']': continue # 문장 끝이 ] 건너뜀\n",
    "# #     if len(sentence) >= 17 : continue\n",
    "\n",
    "        \n",
    "#     if len(preprocess_sentence(sentence)) > 15 : continue\n",
    "        \n",
    "#     if sentence < 30: continue # 15개 넘어가는 문장 건너뜀 # 이건 글자가 30자 미만인거고 단어가 30개 미만이려면?\n",
    "        \n",
    "    corpus.append(preprocess_sentence(sentence)) # corpus에 추가해라 만든 정제함수(preprocess_sentece)에 sentence를 넣어서"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-library",
   "metadata": {},
   "source": [
    "# 평가 데이터셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "chicken-louisiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 평가 데이터셋 분리\n",
    "## tokenize() 함수로 데이터를 tensor로 변환한 후 \n",
    "## sklearn 모듈의 train_test_split()함수를 사용해 훈련데이터와 평가데이터 분리\n",
    "## 단어장의 크기는 12,000이상으로 설정, 총 데이터의 20%를 평가 데이터셋으로 사용\n",
    "## enc_train, enc_val, dec_train, dec_val = <코드 작성>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "painted-denial",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    2    24    19 ...     0     0     0]\n",
      " [    2    31    33 ...     0     0     0]\n",
      " [    2    13   102 ...     0     0     0]\n",
      " ...\n",
      " [    2    24     6 ...     0     0     0]\n",
      " [    4     5    35 ...    96   940     3]\n",
      " [    2    67 11220 ...     0     0     0]] <keras_preprocessing.text.Tokenizer object at 0x7fb26e2bfc50>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'keras_preprocessing.text.Tokenizer'>\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):  \n",
    "    # 텐서플로우에서 제공하는 Tokenizer패키지 생성\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words = 12000, # 전체 단어 개수\n",
    "        filters=' ', # 별도로 전처리 로직을 추가할 수 있지만 이번에는 사용 안 함\n",
    "        oov_token=\"<unk>\" # out-of-vocabulary, 사전에 없었던 단어는 어떤 토큰으로 대체할지\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus) # 우리가 구축한 corpus로부터 Tokenize가 사전을 자동 구축\n",
    "\n",
    "    # 이후 tokenizer를 활용하여 모델에 입력할 데이터셋 구축\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)  # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환\n",
    "\n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기 위한 padding 메소드 제공\n",
    "    # maxlen의 디폴트 값은 None. 이 경우 corpus의 가장 긴 문장을 기준으로 시퀀스 길이가 맞춰짐\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, maxlen= 15, padding='post') # 토큰화 15개 미만?\n",
    "\n",
    "    print(tensor, tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)\n",
    "\n",
    "\n",
    "print(type(tensor))\n",
    "print(type(tokenizer))\n",
    "print('ok')\n",
    "\n",
    "# tensor = [c for c in raw_corpus if c.count(\" \") <13] # 어디다 넣는 건지.........\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "biblical-optimum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "compatible-connectivity",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    2    24    19    42     8    98    26    28     7     3     0     0\n",
      "      0     0     0]\n",
      " [    2    31    33     4 11330     8  3995   282     3     0     0     0\n",
      "      0     0     0]\n",
      " [    2    13   102    31     7     5   998     3     0     0     0     0\n",
      "      0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "print(tensor[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "hungarian-pregnancy",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n",
      "11 : it\n",
      "12 : me\n",
      "13 : my\n",
      "14 : in\n",
      "15 : t\n"
     ]
    }
   ],
   "source": [
    "# 텐서데이터는 모두 정수로 이루어짐\n",
    "# 이 숫자는 tokenizer에 구축된 단어 사전의 인덱스\n",
    "# 단어 사전 어떻게 구축되었는지 확인\n",
    "\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "    \n",
    "    if idx >= 15: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "collected-spiritual",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[ 2 24 19 42  8 98 26 28  7  3  0  0  0  0]\n",
      "[24 19 42  8 98 26 28  7  3  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1] # tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성. 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높음\n",
    "tgt_input = tensor[:, 1:] # tensor에서 <satrt>를 잘라내서 타겟문장 생성\n",
    "\n",
    "print(type(tensor))\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "worth-tribute",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1 # tokenizer가 구축된 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>!\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "respected-anger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skelarn의 train_test_split 함수를 통해 train_data와 test_data분리  \n",
    "# test_size 인자를 조절해주면 설정해준 값 만큼 test dataset 비율 조정 가능\n",
    "# from sklearn.datasets import corpus\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(tensor, tensor, test_size=0.2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "regular-mobile",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (140599, 15)\n",
      "Target Train: (140599, 15)\n",
      "(35150, 15)\n",
      "(35150, 15)\n",
      "[  2   7 176 124 543 528   3   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)\n",
    "print(enc_val.shape)\n",
    "print(dec_val.shape)\n",
    "print(enc_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "essential-nothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 위 과정까지 올바르게 진행했을 경우 아래의 실행결과 확인 가능\n",
    "# print(\"Source Train : \", enc_train.shape)\n",
    "# print(\"Target Train : \", dec_train.shape)\n",
    "\n",
    "# out:\n",
    "# Source Train: (124960, 14)\n",
    "# Target Train: (124960, 14)\n",
    "\n",
    "## 만약 결과가 다르면 동일한 결과를 얻도록 해야함\n",
    "## 학습데이터 개수가 124960 보다 크다면 step3다시 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dominant-comparative",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = preprocess_sentence(sentence)\n",
    "# len(result.split()) < 15\n",
    "# print(len(result.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "formed-export",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2   10  184   75   95  120 9768  974 9769    3    0    0    0    0\n",
      "    0]\n",
      "<start> to yo back then ya shiverin tongue deliverin <end>\n"
     ]
    }
   ],
   "source": [
    "print(tensor[12000])\n",
    "print(corpus[12000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "serious-behalf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "<class 'keras_preprocessing.text.Tokenizer'>\n"
     ]
    }
   ],
   "source": [
    "print(type(tensor.shape))\n",
    "print(type(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "literary-penguin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    2    31    33     4 11330     8  3995   282     3     0     0     0\n",
      "     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "# 생성된 텐서 데이터를 출력\n",
    "print(tensor[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-reason",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "vital-reputation",
   "metadata": {},
   "source": [
    "# Step 5. 인공지능 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "frozen-loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인공지능 만들기\n",
    "# 모델의 embedding size와 hideen size를 조절하여 10epoch안에 val_loss값을 2.2 수준으로 줄일 수 잇는 모델 설계\n",
    "# loss는 아래 제시된 loss함수 그대로 사용\n",
    "\n",
    "# loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "#     from_logits=True, reduction='none')\n",
    "\n",
    "# generate_text(lyricist, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "criminal-parliament",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256 #embedding_size는 워드 벡터의 차원수, 즉 단어가 추상적으로 표현되는 크기\n",
    "hidden_size = 1024 # hidden_size는 모델에 얼마나 많은 일꾼을 둘 것인가 정도로 이해하면 됨\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size, hidden_size)\n",
    "\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "worst-advice",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
       "array([[[ 7.72190688e-05, -2.56716594e-04, -7.82191055e-05, ...,\n",
       "         -1.94737688e-04,  4.41184930e-05,  9.12978940e-05],\n",
       "        [ 6.32292067e-05, -4.12559748e-04, -6.27389411e-04, ...,\n",
       "         -3.93010472e-04, -2.68280390e-04,  1.04974548e-04],\n",
       "        [ 1.51692555e-04, -3.38889018e-04, -7.05388724e-04, ...,\n",
       "         -3.91828216e-04, -5.90193784e-04, -1.77217862e-06],\n",
       "        ...,\n",
       "        [ 6.47143344e-04,  4.30490501e-04, -9.33299714e-04, ...,\n",
       "         -1.97086993e-04, -1.43020751e-03, -4.44302976e-04],\n",
       "        [ 7.42186268e-04,  8.32634454e-04, -1.05870469e-03, ...,\n",
       "          1.76934773e-04, -1.67344220e-03, -3.66209133e-04],\n",
       "        [ 8.45349510e-04,  7.60403578e-04, -1.15285127e-03, ...,\n",
       "          4.85686556e-04, -1.85262680e-03, -5.72123681e-04]],\n",
       "\n",
       "       [[ 7.72190688e-05, -2.56716594e-04, -7.82191055e-05, ...,\n",
       "         -1.94737688e-04,  4.41184930e-05,  9.12978940e-05],\n",
       "        [ 1.38106305e-04, -1.88761813e-04, -2.07371530e-04, ...,\n",
       "         -3.75035394e-04,  1.34504953e-04,  2.41990710e-04],\n",
       "        [ 4.42795601e-04, -5.02541297e-05, -4.42462013e-04, ...,\n",
       "         -2.74333346e-04, -5.42930366e-07, -2.94177098e-05],\n",
       "        ...,\n",
       "        [ 3.87566426e-04, -8.58652289e-04, -1.46214885e-03, ...,\n",
       "          3.49190377e-04, -3.45559354e-04, -2.00987794e-03],\n",
       "        [ 3.09751398e-04, -1.31892576e-03, -1.52503722e-03, ...,\n",
       "          3.52700095e-04, -6.27441681e-04, -2.42047454e-03],\n",
       "        [ 2.32875478e-04, -1.77424995e-03, -1.60651095e-03, ...,\n",
       "          3.08995717e-04, -8.89509392e-04, -2.76567880e-03]],\n",
       "\n",
       "       [[ 7.72190688e-05, -2.56716594e-04, -7.82191055e-05, ...,\n",
       "         -1.94737688e-04,  4.41184930e-05,  9.12978940e-05],\n",
       "        [ 3.44419794e-04, -5.40133740e-04,  3.32289201e-05, ...,\n",
       "         -3.88746383e-04, -9.53589697e-05,  2.57278123e-04],\n",
       "        [ 1.49118525e-04, -4.13186935e-04, -1.23326099e-04, ...,\n",
       "         -2.51040154e-04, -2.18877729e-04,  3.41286446e-04],\n",
       "        ...,\n",
       "        [ 4.07874148e-04, -1.46847207e-03, -1.75258040e-03, ...,\n",
       "          2.01784493e-03, -5.63915877e-04, -9.89579479e-04],\n",
       "        [ 3.69999645e-04, -1.96053926e-03, -1.88993814e-03, ...,\n",
       "          1.92259345e-03, -7.86187244e-04, -1.32827205e-03],\n",
       "        [ 3.15310346e-04, -2.40918994e-03, -2.01263512e-03, ...,\n",
       "          1.76410330e-03, -9.94129106e-04, -1.63851795e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 7.72190688e-05, -2.56716594e-04, -7.82191055e-05, ...,\n",
       "         -1.94737688e-04,  4.41184930e-05,  9.12978940e-05],\n",
       "        [ 1.22710218e-04, -4.36566886e-04, -2.66228337e-04, ...,\n",
       "         -4.17801144e-04,  8.21144858e-05,  2.52346887e-04],\n",
       "        [ 2.37552667e-04, -6.90830289e-04, -4.28862142e-04, ...,\n",
       "         -6.44392450e-04, -5.92513643e-05,  4.62614466e-04],\n",
       "        ...,\n",
       "        [ 5.37945481e-04, -1.83929049e-03, -1.97756873e-03, ...,\n",
       "          7.81792041e-05, -1.22865569e-03, -1.16515136e-03],\n",
       "        [ 4.45599318e-04, -2.25710752e-03, -2.16441997e-03, ...,\n",
       "          1.02607271e-04, -1.40561338e-03, -1.63390522e-03],\n",
       "        [ 3.52755829e-04, -2.64582876e-03, -2.32145935e-03, ...,\n",
       "          9.37230143e-05, -1.56614452e-03, -2.03642924e-03]],\n",
       "\n",
       "       [[ 7.72190688e-05, -2.56716594e-04, -7.82191055e-05, ...,\n",
       "         -1.94737688e-04,  4.41184930e-05,  9.12978940e-05],\n",
       "        [ 6.23977248e-05, -4.17720585e-04, -2.89687858e-04, ...,\n",
       "         -3.27306596e-04,  2.00011869e-04,  2.48583092e-04],\n",
       "        [-8.30443751e-05, -3.54241522e-04, -4.31902445e-04, ...,\n",
       "         -4.62031021e-04,  3.37486534e-04,  2.71107478e-04],\n",
       "        ...,\n",
       "        [ 4.49626204e-05, -2.25485535e-03, -1.55690790e-03, ...,\n",
       "          3.20838357e-04, -1.36026368e-03, -2.02815607e-03],\n",
       "        [ 1.86959405e-05, -2.66354065e-03, -1.77356426e-03, ...,\n",
       "          2.85506452e-04, -1.51282095e-03, -2.33856216e-03],\n",
       "        [-4.24677546e-06, -3.01965163e-03, -1.95822329e-03, ...,\n",
       "          2.42042981e-04, -1.64325314e-03, -2.60065380e-03]],\n",
       "\n",
       "       [[ 7.72190688e-05, -2.56716594e-04, -7.82191055e-05, ...,\n",
       "         -1.94737688e-04,  4.41184930e-05,  9.12978940e-05],\n",
       "        [ 7.17883586e-06, -2.31415688e-04, -4.18065974e-06, ...,\n",
       "         -3.35018209e-04,  1.04196239e-04,  9.54281641e-05],\n",
       "        [-3.87341024e-05,  6.34912722e-05, -6.84205879e-05, ...,\n",
       "         -4.33160058e-05,  2.54788138e-05,  9.35440985e-05],\n",
       "        ...,\n",
       "        [-1.14983755e-04, -3.35627794e-03, -2.03032466e-03, ...,\n",
       "          5.47916396e-04, -1.67350005e-03, -2.97882734e-03],\n",
       "        [-1.22460144e-04, -3.60450172e-03, -2.15239846e-03, ...,\n",
       "          4.66644560e-04, -1.77956198e-03, -3.14005581e-03],\n",
       "        [-1.22343044e-04, -3.81299364e-03, -2.24150997e-03, ...,\n",
       "          3.92161979e-04, -1.86842354e-03, -3.27632390e-03]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model에 데이터 태우기\n",
    "\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "educated-claim",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  3072256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  12301025  \n",
      "=================================================================\n",
      "Total params: 29,012,961\n",
      "Trainable params: 29,012,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "atlantic-xerox",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "686/686 [==============================] - 85s 124ms/step - loss: 3.6197\n",
      "Epoch 2/10\n",
      "686/686 [==============================] - 86s 125ms/step - loss: 3.1207\n",
      "Epoch 3/10\n",
      "686/686 [==============================] - 86s 126ms/step - loss: 2.9225\n",
      "Epoch 4/10\n",
      "686/686 [==============================] - 86s 126ms/step - loss: 2.7704\n",
      "Epoch 5/10\n",
      "686/686 [==============================] - 89s 129ms/step - loss: 2.6384\n",
      "Epoch 6/10\n",
      "686/686 [==============================] - 89s 129ms/step - loss: 2.5193\n",
      "Epoch 7/10\n",
      "686/686 [==============================] - 88s 128ms/step - loss: 2.4103\n",
      "Epoch 8/10\n",
      "686/686 [==============================] - 88s 128ms/step - loss: 2.3150\n",
      "Epoch 9/10\n",
      "686/686 [==============================] - 87s 127ms/step - loss: 2.2120\n",
      "Epoch 10/10\n",
      "686/686 [==============================] - 87s 127ms/step - loss: 2.1183\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb2083f6950>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 학습 시작\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "clean-dispute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "# generate_text 함수는 모델에게 시작 문장을 전달하면 모델이 시작 문장을 바탕으로 작문을 진행함\n",
    "\n",
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해 입력받은 init_sentence도 일단 텐서로 변환\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "    \n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성 해야함\n",
    "    while True:\n",
    "        predict = model(test_tensor) # 입력받은 문장의 텐서를 입력\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] # 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됨\n",
    "        \n",
    "        # 모델이 새롭게 예측한 단어를 입력 문장 뒤에 붙여줌\n",
    "        test_tensor = tf.concat([test_tensor, \n",
    "                                 tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        \n",
    "        # 모델이 <end>를 예측했거나, max_len에 도달하지 않았으면 while루프를 또 돌면서 다음 단어를 예측\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "            \n",
    "    generated = \"\"\n",
    "              \n",
    "    # 생성된 tensor안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "        \n",
    "    return generated # 이것이 최종적으로 모델이 생성한 자연어 문장\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "purple-globe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> love is a beautiful thing <end> '"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 위 문장 생성 함수 실행\n",
    "generate_text(model, tokenizer, init_sentence=\"<start> love\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "weekly-ceramic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> peace to <unk> , <end> '"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 위 문장 생성 함수 실행\n",
    "generate_text(model, tokenizer, init_sentence=\"<start> peace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "molecular-hacker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> home of the moon <end> '"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 위 문장 생성 함수 실행\n",
    "generate_text(model, tokenizer, init_sentence=\"<start> home\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geographic-founder",
   "metadata": {},
   "source": [
    "# 총평"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blessed-arlington",
   "metadata": {},
   "source": [
    "## 루브릭 평가  \n",
    "### 1. 그럴듯한 문장으로 생성되는가?\n",
    "- 잘 생성이 안되는 것도 있고 제대로 생성되는 문장도 있다. 아마 내가 뭘 잘못해서 그런 것 같은데 코드를 거의 복사 붙여넣기를 한 것이라 뭐가 어디서 잘못됐는지 못찾겠다.\n",
    "- 아마 토큰화부분에서 maxlen을 지정해주지 않고 파이썬 코드를 통해 토큰화를 15개 미만으로 해야하는 것 같은데 어떻게 하는지 잘 모르겠다. 저번에 다른 사람이 이렇게 했다고 알려줘서 보고 해봤지만 잘 모르겠다.(내가 알려주신 분의 코드를 제대로 못 본 것 같다.)\n",
    "- 그래도 작동이 되는건 아마 end부분이 제일 끝에 있는 것이 아니여서 maxlen을 지정해도 작동이 되는 것 같다..?\n",
    "  \n",
    "### 2. 특수문자  제거, 토크나이저 생성, 패딩처리 등의 과정이 빠짐없이 진행되었는가?\n",
    "- 복사 붙여넣기 해서 빠진 부분은 없는 것 같다.\n",
    "    \n",
    "### 3. validation loss까 2.2이하로 낮아졌는가?\n",
    "- 돌려보니 최종적으로 2.2이하로 낮아졌다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
